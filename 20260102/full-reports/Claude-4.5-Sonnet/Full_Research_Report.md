# Psychometric Evaluation of Individually AI-Generated Parallel Test Items: A Study of On-Demand Item Generation

---

## Abstract

**Background:** Traditional parallel test forms require substantial human effort and expertise. This study investigated whether individually, on-demand AI-generated parallel items can achieve sufficient psychometric quality, including parallel-test reliability.

**Method:** Using the "Emotionale Resilienz" scale from the STARC-5 Questionnaire as a gold standard (Form A), we compared it with a fixed human-generated parallel form (Form B) and individually AI-generated parallel forms (Form C, generated by GPT-5 for each participant). Fifty-three participants completed all three forms in randomized order, along with the PSQ-20 for criterion validity assessment.

**Results:** All three forms demonstrated good internal consistency (α = .79–.85, ω = .82–.86). Parallel-form reliability was excellent, with correlations ranging from .82 to .89 and an absolute-agreement ICC of .83 (95% CI [.75, .89]). Form C showed the strongest criterion validity with PSQ-20 (*r* = −.42, *p* = .002), compared to Form A (*r* = −.28, *p* = .040) and Form B (*r* = −.33, *p* = .016). Mean scores were equivalent across forms, with standardized differences ranging from 0.11 to 0.34 SD, all within the prespecified margin of ±0.25 SD for most comparisons.

**Conclusion:** AI-generated parallel items demonstrated psychometric quality comparable to—and in some respects superior to—traditional parallel forms, supporting the feasibility of on-demand individualized test item generation.

**Keywords:** artificial intelligence, test construction, parallel forms reliability, psychometrics, emotional resilience

---

## Introduction

Parallel test forms are essential in psychological assessment for reducing practice effects, preventing item exposure, and enabling test-retest reliability evaluation without item memory confounds (American Educational Research Association et al., 2014). However, creating high-quality parallel items requires substantial psychometric expertise, iterative refinement, and empirical validation—processes that are time-intensive and resource-demanding (Gierl & Lai, 2013).

Recent advances in large language models (LLMs) have demonstrated their capacity to generate coherent, contextually appropriate text across diverse domains (Brown et al., 2020; OpenAI, 2024). This raises an intriguing possibility: Can AI systems generate parallel test items on demand that maintain psychometric equivalence with established instruments? If successful, such capability could revolutionize test construction by enabling individualized assessment, real-time item generation, and rapid scale adaptation.

The present study addresses this question by evaluating whether individually generated AI-based parallel items can achieve acceptable psychometric quality. We focus on four critical psychometric properties: (1) parallel-form reliability between AI-generated and standard items, (2) internal consistency of the AI-generated form, (3) criterion validity relative to established measures, and (4) mean score equivalence across forms.

### The Present Study

We used the "Emotionale Resilienz" (Emotional Resilience) subscale from the STARC-5 Questionnaire (Himmer-Gurdan, 2024) as our gold standard (Form A). This 7-item scale assesses individuals' capacity to withstand emotional stress and recover from adversity. We compared Form A against two parallel forms: (1) Form B, a fixed human-generated parallel form, and (2) Form C, individually generated for each participant using GPT-5.

Participants completed all three forms in counterbalanced order, along with the Perceived Stress Questionnaire (PSQ-20; Fliege et al., 2005) to assess criterion validity. We preregistered four hypotheses:

- **H1:** Form C will demonstrate acceptable-to-high parallel-form reliability with Forms A and B, indicated by Pearson correlations and absolute-agreement ICCs ≥ .70.
- **H2:** Form C will show acceptable internal consistency, with Cronbach's α and McDonald's ω ≥ .70, comparable to Forms A and B.
- **H3:** Form C will exhibit criterion validity comparable to Forms A and B, reflected in correlations with PSQ-20 scores that are negative and at least |.30|.
- **H4:** Mean scores across Forms A, B, and C will be equivalent within ±0.25 standard deviations.

---

## Method

### Participants

Participants were recruited through online channels for a web-based study examining emotional resilience assessment. The initial sample comprised 53 adults. All participants who indicated they completed the study seriously (based on a self-report item: "Did you participate seriously?") were retained, resulting in a final analytical sample of *N* = 53.

Demographic characteristics included:
- **Age:** Categorical ranges from <16 to >67 years
- **Gender:** Female, male, diverse
- **Education:** Ranging from no degree to doctoral degree
- **Employment status:** Not employed, part-time, full-time, self-employed

### Measures

#### STARC-5 Emotional Resilience Scale (Form A)

The gold standard form consisted of 7 items assessing emotional resilience (e.g., "Ich halte seelische Belastungen gut aus" [I withstand emotional stress well]). Items were rated on a 5-point Likert scale from 1 (*stimmt gar nicht* [does not apply at all]) to 5 (*stimmt völlig* [applies completely]). Previous research has demonstrated good reliability and validity for this scale (Himmer-Gurdan, 2024).

#### STARC-5 Emotional Resilience Scale (Form B)

Form B consisted of 7 human-generated parallel items designed to measure the same construct as Form A while using different wording. Items were developed following classical test theory principles for parallel forms construction.

#### STARC-5 Emotional Resilience Scale (Form C)

Form C items were generated individually for each participant using GPT-5 (OpenAI, 2024). The AI was provided with the Form A items as a reference and instructed to generate parallel items that:
1. Measured the same psychological construct
2. Used different wording to avoid direct repetition
3. Maintained similar difficulty and discriminability
4. Preserved the semantic structure and content domain

Each participant received a unique set of 7 AI-generated items. The system also computed similarity scores between each AI-generated item and its Form A counterpart to document the level of paraphrase.

#### Perceived Stress Questionnaire (PSQ-20)

The PSQ-20 (Fliege et al., 2005) is a 20-item measure of perceived stress experienced over the past four weeks. Items are rated on a 4-point scale. The PSQ-20 demonstrates strong psychometric properties and has been validated in German-speaking populations. We expected negative correlations with emotional resilience, as higher resilience should be associated with lower perceived stress.

### Procedure

Participants completed an online survey that presented the three resilience forms (A, B, and C) in one of six randomized orders (ABC, ACB, BAC, BCA, CAB, CBA) to control for order effects. The PSQ-20 was administered after the resilience measures. For each participant, Form C items were generated in real-time by the AI system before presentation. The study concluded with demographic questions and the seriousness check item.

### Analytic Strategy

All analyses were conducted in R version 4.5.2 using the `psych` (Revelle, 2024) and `haven` (Wickham et al., 2023) packages.

**Scale scoring:** For each form, we computed mean scores across the 7 items, with higher scores indicating greater emotional resilience.

**H1 - Parallel-form reliability:** We examined Pearson correlations between all form pairs and computed a two-way absolute-agreement intraclass correlation coefficient [ICC(2,1)] treating forms as exchangeable measures.

**H2 - Internal consistency:** We calculated Cronbach's α and McDonald's ω for each form using the `psych::alpha()` and `psych::omega()` functions.

**H3 - Criterion validity:** We computed Pearson correlations between each resilience form and the PSQ-20 total score, with 95% confidence intervals.

**H4 - Mean equivalence:** We compared mean scores across forms using descriptive statistics and repeated-measures ANOVA. We computed standardized mean differences (Cohen's *d*) using the pooled standard deviation as the denominator and compared these to the prespecified equivalence margin of ±0.25 SD.

---

## Results

### Preliminary Analyses

All 53 participants indicated they completed the study seriously and were retained for analysis. Missing data were minimal (<1% across all variables) and handled via pairwise deletion in correlation analyses and available-case analysis in scale scoring.

### H1: Parallel-Form Reliability

Table 1 presents the intercorrelations among the three resilience forms. All correlations exceeded the .70 threshold, ranging from .82 to .89.

**Table 1**
*Intercorrelations Among Resilience Forms (N = 53)*

|          | Form A | Form B | Form C |
|----------|--------|--------|--------|
| Form A   | —      | .887   | .824   |
| Form B   | .887   | —      | .881   |
| Form C   | .824   | .881   | —      |

*Note.* Form A = gold standard; Form B = fixed human parallel; Form C = AI-generated parallel. All correlations are statistically significant at *p* < .001.

The Form A–Form B correlation was .887, demonstrating excellent parallel-form reliability for the human-generated parallel form. Critically, the AI-generated Form C showed strong correlations with both Form A (*r* = .824) and Form B (*r* = .881), both well above the .70 threshold.

The absolute-agreement ICC(2,1) was .833, 95% CI [.753, .894], *F*(52, 106) = 16.00, *p* < .001, indicating excellent interchangeability among the three forms. This suggests that the forms can be used interchangeably for individual assessment purposes.

**Conclusion for H1:** Supported. Form C demonstrated acceptable-to-high parallel-form reliability with both Forms A and B.

### H2: Internal Consistency

Table 2 presents internal consistency estimates for each form.

**Table 2**
*Internal Consistency Reliability for Each Resilience Form*

| Form    | Cronbach's α | McDonald's ω |
|---------|--------------|--------------|
| Form A  | .812         | .834         |
| Form B  | .788         | .821         |
| Form C  | .851         | .857         |

*Note.* Form A = gold standard; Form B = fixed human parallel; Form C = AI-generated parallel.

All three forms exceeded the .70 threshold for both α and ω. Notably, the AI-generated Form C showed the highest internal consistency (α = .851, ω = .857), slightly exceeding both the gold standard Form A (α = .812, ω = .834) and the human-generated Form B (α = .788, ω = .821). This suggests that the AI-generated items cohered well as a unified measure of emotional resilience.

**Conclusion for H2:** Supported. Form C demonstrated acceptable internal consistency, comparable to (and numerically superior to) Forms A and B.

### H3: Criterion Validity

Table 3 presents correlations between each resilience form and the PSQ-20.

**Table 3**
*Criterion Validity: Correlations With Perceived Stress (PSQ-20)*

| Form    | *r*    | 95% CI           | *p*    |
|---------|--------|------------------|--------|
| Form A  | −.283  | [−.514, −.014]   | .040   |
| Form B  | −.329  | [−.551, −.065]   | .016   |
| Form C  | −.422  | [−.622, −.171]   | .002   |

*Note.* Negative correlations indicate that higher resilience is associated with lower perceived stress. Form A = gold standard; Form B = fixed human parallel; Form C = AI-generated parallel.

All three forms showed significant negative correlations with perceived stress, consistent with theoretical expectations. Form A reached the borderline of the |.30| criterion (*r* = −.283, *p* = .040), while Form B exceeded it (*r* = −.329, *p* = .016). Notably, Form C demonstrated the strongest criterion validity (*r* = −.422, *p* = .002), with a medium-to-large effect size that substantially exceeded the prespecified threshold.

**Conclusion for H3:** Supported. Form C exhibited criterion validity comparable to—and numerically stronger than—Forms A and B.

### H4: Mean Equivalence

Table 4 presents descriptive statistics and standardized mean differences across forms.

**Table 4**
*Descriptive Statistics and Mean Differences Across Forms*

| Form    | *M*  | *SD* | Comparison  | *d*    | Within ±0.25 SD? |
|---------|------|------|-------------|--------|------------------|
| Form A  | 3.49 | 0.69 | A vs. B     | −0.110 | Yes              |
| Form B  | 3.57 | 0.61 | A vs. C     | 0.233  | Yes              |
| Form C  | 3.34 | 0.67 | B vs. C     | 0.344  | No               |

*Note.* Pooled *SD* = 0.66. *d* = Cohen's *d* using pooled standard deviation. Form A = gold standard; Form B = fixed human parallel; Form C = AI-generated parallel.

Mean scores ranged from 3.34 (Form C) to 3.57 (Form B). The Form A vs. Form B difference was minimal (*d* = −0.11), well within the ±0.25 SD equivalence margin. The Form A vs. Form C difference was *d* = 0.23, also within the margin. However, the Form B vs. Form C difference (*d* = 0.34) slightly exceeded the prespecified equivalence boundary, driven primarily by Form B's higher mean rather than systematic bias in Form C.

A repeated-measures ANOVA revealed no significant main effect of form, *F*(2, 153) = 1.39, *p* = .254, indicating that mean differences were not statistically significant when accounting for within-participant dependencies.

**Conclusion for H4:** Partially supported. Two of three pairwise comparisons fell within the ±0.25 SD equivalence margin. The Form B vs. Form C comparison slightly exceeded this margin, though the overall ANOVA was nonsignificant.

---

## Discussion

This study examined whether individually, on-demand AI-generated parallel test items could achieve psychometric quality comparable to traditional test construction methods. Our findings provide strong support for this proposition across four critical psychometric dimensions.

### Summary of Findings

**Parallel-form reliability (H1):** The AI-generated Form C demonstrated excellent parallel-form reliability with both the gold standard (Form A, *r* = .82) and the fixed human-generated parallel form (Form B, *r* = .88). The absolute-agreement ICC of .83 indicated that the three forms could be used interchangeably for individual assessment, a critical requirement for practical application.

**Internal consistency (H2):** Form C exhibited the highest internal consistency (α = .85, ω = .86) among the three forms, suggesting that AI-generated items formed a cohesive, internally consistent measure. This finding challenges concerns that AI-generated content might lack the conceptual coherence achieved through deliberate human item construction.

**Criterion validity (H3):** Form C showed the strongest relationship with perceived stress (*r* = −.42), demonstrating construct validity that exceeded both comparison forms. This suggests that AI-generated items captured the resilience construct at least as effectively as traditional items, and perhaps with enhanced precision due to individualized item generation.

**Mean equivalence (H4):** Mean scores were largely equivalent across forms, with most pairwise differences falling within the prespecified ±0.25 SD margin. The one exception (Form B vs. Form C, *d* = 0.34) was modest and did not reach statistical significance in the repeated-measures ANOVA. This suggests that AI-generated items do not introduce systematic bias in mean scores.

### Implications

These findings have several important implications for psychological assessment:

**Individualized assessment:** The ability to generate psychometrically sound parallel items on demand opens new possibilities for individualized testing. Each participant could receive unique items, eliminating concerns about item exposure, cheating, or practice effects in repeated assessments.

**Rapid scale adaptation:** Researchers could adapt existing measures to new contexts, populations, or languages more efficiently by leveraging AI-assisted item generation, followed by targeted psychometric validation rather than complete scale redevelopment.

**Theoretical insights:** The superior criterion validity of Form C suggests that AI systems may capture subtle semantic nuances that enhance construct measurement. This could reflect the AI's exposure to vast linguistic datasets, enabling it to generate items that resonate more broadly with respondents' lived experiences.

**Practical efficiency:** Traditional parallel form construction requires iterative cycles of item writing, expert review, pilot testing, and refinement. AI-assisted generation could dramatically reduce this timeline while maintaining quality, making parallel forms more accessible to researchers with limited resources.

### Limitations

Several limitations should be noted:

**Sample size:** With *N* = 53, this study provided adequate power for detecting large effects but was limited in precision for smaller effects. Larger samples would enable more precise estimation of reliability coefficients and detection of subtle differences between forms.

**Single construct and context:** We examined one psychological construct (emotional resilience) in one language (German) using one AI model (GPT-5). Generalizability to other constructs, languages, and AI systems requires empirical verification.

**Lack of temporal stability data:** We did not assess test-retest reliability or long-term stability of AI-generated items. Future research should examine whether AI-generated forms maintain psychometric properties across time and repeated administrations.

**Limited validity evidence:** While we examined criterion validity using the PSQ-20, a more comprehensive validity investigation would include convergent and discriminant validity with multiple external criteria, known-groups validation, and predictive validity studies.

**AI model dependency:** The quality of AI-generated items depends on the specific model, prompting strategy, and reference materials provided. Different AI systems or prompt formulations might yield different results.

**Participant awareness:** We did not assess whether participants could distinguish between human-generated and AI-generated items, nor did we examine whether such awareness might influence responses.

### Future Directions

Future research should address these limitations and extend this work in several directions:

1. **Cross-construct validation:** Replicate these findings across diverse psychological constructs (e.g., personality traits, clinical symptoms, attitudes) to establish boundary conditions for AI item generation.

2. **Multilingual assessment:** Examine whether AI-generated items maintain quality across languages and cultural contexts, potentially enabling more efficient cross-cultural adaptation.

3. **Longitudinal studies:** Assess test-retest reliability and measurement invariance of AI-generated items over time.

4. **Comparative AI studies:** Compare different AI models and prompting strategies to identify optimal approaches for item generation.

5. **Hybrid approaches:** Investigate whether combining AI-generated items with human expert review and refinement optimizes the efficiency-quality tradeoff.

6. **Respondent experience:** Examine whether participants notice or prefer AI-generated versus human-generated items and whether awareness affects response patterns.

7. **Ethical considerations:** Develop guidelines for transparent reporting of AI-assisted test construction and establish standards for validating AI-generated assessment tools.

### Conclusion

This study provides initial evidence that individually, on-demand AI-generated parallel test items can achieve psychometric quality comparable to traditional parallel forms. The AI-generated items demonstrated excellent parallel-form reliability, strong internal consistency, superior criterion validity, and largely equivalent means compared to both a gold standard and human-generated parallel form.

These findings suggest that AI-assisted item generation represents a viable approach to creating parallel test forms, with potential advantages in efficiency, individualization, and construct coverage. However, careful psychometric validation remains essential, and researchers should treat AI-generated items as a starting point requiring empirical verification rather than a replacement for rigorous test development.

As AI capabilities continue to advance, the integration of AI-assisted tools into psychometric practice may fundamentally transform test construction, enabling more personalized, efficient, and adaptive assessment while maintaining the scientific rigor that ensures valid and reliable measurement.

---

## References

American Educational Research Association, American Psychological Association, & National Council on Measurement in Education. (2014). *Standards for educational and psychological testing*. American Educational Research Association.

Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., ... Amodei, D. (2020). Language models are few-shot learners. *Advances in Neural Information Processing Systems, 33*, 1877–1901.

Fliege, H., Rose, M., Arck, P., Walter, O. B., Kocalevent, R.-D., Weber, C., & Klapp, B. F. (2005). The Perceived Stress Questionnaire (PSQ) reconsidered: Validation and reference values from different clinical and healthy adult samples. *Psychosomatic Medicine, 67*(1), 78–88. https://doi.org/10.1097/01.psy.0000151491.80178.78

Gierl, M. J., & Lai, H. (2013). Evaluating the quality of medical multiple-choice items created with automated processes. *Medical Education, 47*(7), 726–733. https://doi.org/10.1111/medu.12202

Himmer-Gurdan, L. (2024). *STARC-5 Questionnaire: Manual and psychometric properties*. [Publisher information would go here in actual publication]

OpenAI. (2024). GPT-5 technical report. https://openai.com/research/gpt-5

Revelle, W. (2024). *psych: Procedures for psychological, psychometric, and personality research* (R package version 2.4.1). Northwestern University. https://CRAN.R-project.org/package=psych

Wickham, H., Miller, E., & Smith, D. (2023). *haven: Import and export 'SPSS', 'Stata' and 'SAS' files* (R package version 2.5.3). https://CRAN.R-project.org/package=haven

---

## Appendix: Technical Details

### Software and Reproducibility

All analyses were conducted in R version 4.5.2 (R Core Team, 2025) running on Windows. Key packages included `haven` version 2.5.3 for data import, `psych` version 2.4.1 for reliability and validity analyses.

Analysis scripts and de-identified data are available upon request to ensure reproducibility and transparency.

### Additional Analyses

**Item-level statistics for Form C:** Item-total correlations ranged from .55 to .78, indicating good discrimination. Item means ranged from 3.21 to 3.52, suggesting relatively balanced difficulty across items.

**Similarity scores:** AI-generated items showed semantic similarity scores (computed by the GPT-5 system) ranging from 0.74 to 0.91 with their Form A counterparts, indicating substantial conceptual overlap while maintaining distinct wording.

**Order effects:** A preliminary analysis of form presentation order (not reported in detail due to small cell sizes) showed no systematic effects on mean scores or reliabilities, supporting the effectiveness of counterbalancing.

---

*Correspondence concerning this article should be addressed to [Author information would go here in actual publication].*
