---
title: "Psychometric Evaluation of AI-Generated Parallel Items for the STARC-5 Emotionale Resilienz Scale"
date: "2026-01-01"
nlss_version: "1.0.0"
---

# Psychometric Evaluation of AI-Generated Parallel Items for the STARC-5 Emotionale Resilienz Scale

`<user-placeholder>`  
`<affiliation-placeholder>`

`<email-placeholder>`

2026-01-01

## Abstract

This study examined whether individually generated AI parallel items (Form C) for the STARC-5 Emotionale Resilienz scale achieve psychometric quality comparable to a gold standard form (A) and a fixed human parallel form (B). Using N = 53, we recomputed form scores from item responses, estimated internal consistency (alpha/omega), evaluated parallel-form reliability (Pearson correlations and absolute-agreement ICC), tested criterion validity via correlations with PSQ-20 scores, and assessed mean-score equivalence using paired TOST with a ±0.25 SD margin. Form C demonstrated acceptable to high reliability and strong parallel-form correlations with A and B, with meaningful negative correlations to PSQ-20. Equivalence testing supported A–B mean equivalence but not A–C or B–C within the predefined margin.

# Psychometric Evaluation of AI-Generated Parallel Items for the STARC-5 Emotionale Resilienz Scale

## Introduction

Parallel-form reliability evaluates the degree to which different item sets that sample the same construct yield similar scores, typically assessed by correlations between form scores in the same sample.citeturn1search3 This framework is central to determining whether AI-generated items can function as true parallel forms. Recent evidence in educational measurement suggests that AI-generated items can approach human-authored items in psychometric properties, but often require expert review and validation to ensure quality and construct alignment.citeturn2search8 The present study extends this line of work to psychometric scale development, testing whether on-demand AI-generated parallel items for STARC-5 match a human parallel form and relate to criterion measures as expected.

Criterion validity was evaluated against the Perceived Stress Questionnaire (PSQ), a validated measure of perceived stress with established psychometric properties and factor structure.citeturn1search2 For equivalence testing of mean scores, the two one-sided tests (TOST) framework provides a principled method to establish statistical equivalence within a pre-specified margin.citeturn2search5

## Method

### Participants

The sample included 53 participants from an online study. All cases were marked as serious participation (ernsthaftigkeit = 1), so no exclusions were applied.

### Measures

- STARC-5 Emotionale Resilienz (Form A; gold standard), human parallel Form B, and AI-generated parallel Form C. Each form included 7 items on a 1–5 scale. Total scores were recomputed as mean item scores (starc_5_A_recalc, starc_5_B_recalc, starc_5_C_recalc).
- PSQ-20 (criterion measure): 20 items on a 1–4 scale; reverse-scored items were 1, 4, 6, 8, 12, 14, 16, 19. Total score was recomputed as a mean (psq_20_recalc) following established PSQ structure.citeturn1search2

### Procedure

Participants completed the three STARC-5 forms in randomized order (ABC, ACB, BAC, BCA, CAB, CBA), followed by PSQ-20 and demographics. AI-generated items for Form C were produced individually per participant.

### Analytic Strategy

- Internal consistency: Cronbach’s alpha and McDonald’s omega for each form.
- Parallel-form reliability: Pearson correlations among A/B/C totals; absolute-agreement ICC(2,1) across the three forms.
- Criterion validity: correlations between each form total and PSQ-20 (expected negative associations).
- Mean equivalence: paired TOST for A–B, A–C, and B–C with bounds ±0.25 SD (pooled SD per pair).citeturn2search5

## Results

### Descriptive Statistics

**Table 1**

*Descriptive Statistics for Recomputed Form Scores*

| Measure | n | M | SD | Min | Max |
| --- | --- | --- | --- | --- | --- |
| STARC-5 A (recalc) | 53 | 3.49 | 0.69 | 1.00 | 5.00 |
| STARC-5 B (recalc) | 53 | 3.57 | 0.61 | 1.57 | 5.00 |
| STARC-5 C (recalc) | 53 | 3.34 | 0.67 | 1.00 | 5.00 |
| PSQ-20 (recalc) | 53 | 2.36 | 0.52 | 1.15 | 3.60 |

*Note.* Scores are mean item scores.

### Preliminary Analyses: Reliability

**Table 2**

*Internal Consistency by Form*

| Scale | k | Alpha | Omega |
| --- | --- | --- | --- |
| STARC-5 A | 7 | 0.81 | 0.83 |
| STARC-5 B | 7 | 0.79 | 0.82 |
| STARC-5 C | 7 | 0.85 | 0.86 |
| PSQ-20 | 20 | 0.92 | 0.92 |

### Hypothesis Testing

**H1 (parallel-form reliability).** Form C correlated strongly with Forms A and B (r = .82 with A; r = .88 with B), exceeding the .70 threshold. The absolute-agreement ICC across A/B/C was 0.83 (95% CI [0.76, 0.89]), supporting high overall parallel-form agreement. These analyses align with the recommended approach for assessing parallel forms reliability.citeturn1search3

**H2 (internal consistency).** Form C showed alpha = .85 and omega = .86, comparable to A and B, exceeding the .70 criterion.

**H3 (criterion validity).** Correlations with PSQ-20 were negative and at least |.30|: A (r = -.34), B (r = -.39), and C (r = -.53), supporting criterion validity relative to stress as measured by PSQ.citeturn1search2

**H4 (mean equivalence).** Paired TOST tests with ±0.25 SD bounds supported equivalence for A vs. B, but not for A vs. C or B vs. C. The TOST framework is appropriate for testing equivalence within a smallest effect size of interest.citeturn2search5

**Table 3**

*Parallel-Form Correlations and ICC*

| Comparison | r | 95% CI | p |
| --- | --- | --- | --- |
| A vs B | .89 | [.81, .93] | < .001 |
| A vs C | .82 | [.71, .89] | < .001 |
| B vs C | .88 | [.80, .93] | < .001 |
| A vs PSQ-20 | -.34 | [-.56, -.08] | = .012 |
| B vs PSQ-20 | -.39 | [-.60, -.14] | = .004 |
| C vs PSQ-20 | -.53 | [-.70, -.30] | < .001 |

*Note.* ICC(2,1) absolute agreement across A/B/C = 0.83, 95% CI [0.76, 0.89].

**Table 4**

*Paired TOST Equivalence (±0.25 SD)*

| Pair | Mean Diff | Delta | 90% CI Low | 90% CI High | TOST p | Equivalent |
| --- | --- | --- | --- | --- | --- | --- |
| A vs B | -0.073 | 0.163 | -0.146 | 0.001 | 0.023 | Yes |
| A vs C | 0.154 | 0.171 | 0.060 | 0.247 | 0.380 | No |
| B vs C | 0.226 | 0.160 | 0.153 | 0.300 | 0.932 | No |

*Note.* Equivalence bounds defined as ±0.25 × pooled SD per pair; TOST uses 90% CI and α = .05.citeturn2search5

## Discussion

### Summary of Findings

AI-generated Form C demonstrated strong parallel-form reliability and internal consistency comparable to Forms A and B, with criterion validity consistent with expectations against PSQ-20. These findings align with broader evidence that AI-generated assessment content can achieve psychometric properties similar to human-authored items when carefully evaluated.citeturn2search8 However, equivalence testing indicated that Form C mean scores were not equivalent to A or B within the ±0.25 SD margin, suggesting potential systematic differences in scale level despite strong reliability.

### Limitations

The sample size was modest (N = 53), limiting precision for equivalence bounds. The equivalence margin was operationalized as ±0.25 pooled SD per pair, and alternative definitions may yield different conclusions. The STARC-5 instrument is cited from study materials; no publicly indexed source was identified during the literature scan.

### Implications

On-demand AI-generated parallel items can achieve strong reliability and criterion validity, but equivalence in mean levels is not guaranteed. For applied use, AI-generated forms may require calibration or anchoring to ensure score comparability across administrations.

### Future Directions

Future work should preregister equivalence margins, validate with larger samples, and explore item calibration approaches to align AI-generated forms with established benchmarks.

## Conclusion

Form C meets reliability and criterion validity benchmarks, supporting the feasibility of AI-generated parallel items for STARC-5. Mean-level equivalence remains a challenge under the current margin, indicating the need for calibration strategies before AI-generated forms are used interchangeably with established forms.

## References

Fliege, H., Rose, M., Arck, P., Walter, O. B., Kocalevent, R.-D., Weber, C., & Klapp, B. F. (2005). The Perceived Stress Questionnaire (PSQ) reconsidered: Validation and reference values from different clinical and healthy adult samples. *Psychosomatic Medicine, 67*(1), 78–88. doi:10.1097/01.psy.0000151491.80178.78

Lakens, D. (2017). Equivalence tests: A practical primer for t tests, correlations, and meta-analyses. *Social Psychological and Personality Science, 8*(4), 355–362. doi:10.1177/1948550617697177

Parallel forms reliability. (2008). In N. J. Salkind (Ed.), *Encyclopedia of Research Design*. SAGE Publications. doi:10.4135/9781412961288.n301

Yilmaz, M., et al. (2025). Comparison of AI-generated and clinician-designed multiple-choice questions in emergency medicine exam: A psychometric analysis. *BMC Medical Education*. doi:10.1186/s12909-025-07528-6
