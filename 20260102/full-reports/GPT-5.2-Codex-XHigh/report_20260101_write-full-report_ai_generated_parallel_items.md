---
name: metaskill-report
nlss_version: "1.0.0"
---

# Psychometric Evaluation of AI-Generated Parallel Items for STARC-5 Emotional Resilience

`<user-placeholder>`  

`<affiliation-placeholder>`

`<email-placeholder>`

January 1, 2026

## Abstract

This study evaluated whether on-demand, AI-generated parallel items (Form C) for the STARC-5 Emotional Resilience scale achieve psychometric quality comparable to a human-authored parallel form (Form B) and the original form (Form A). An online sample of N = 53 participants completed all three forms in randomized order plus the PSQ-20 for criterion validity. All scale scores were recomputed as mean item scores; PSQ-20 reverse items were re-scored. Internal consistency (alpha, omega), parallel-form reliability (Pearson r and ICC[2,1] agreement), and criterion validity (correlations with PSQ-20) were estimated. Mean equivalence across forms was tested using paired two one-sided tests (TOST) with bounds set to plus/minus 0.25 of the SD of each pairwise difference. Form C showed high internal consistency (alpha = .85, omega = .86) and strong correlations with Forms A and B (r = .82 to .88) and ICC agreement (.81 to .88). Criterion validity was supported: all STARC-5 forms correlated negatively with PSQ-20 (r = -.34 to -.53). Equivalence of means was not supported for any pair; Form C scores were lower than A and B beyond the prespecified bounds. AI-generated parallel items can yield reliable and valid scores, but mean-level equivalence may require further calibration.

*Keywords:* automated item generation; parallel forms reliability; emotional resilience; psychometrics; PSQ-20

# Psychometric Evaluation of AI-Generated Parallel Items for STARC-5 Emotional Resilience

Automated item generation (AIG) has been used to expand item banks and improve test efficiency while preserving psychometric quality, with evidence that algorithmically generated items can be reliable and valid when carefully designed and evaluated (Lai et al., 2016; Shin & Gierl, 2024; von Davier, 2018). A core requirement for interchangeable forms is parallel-forms reliability, which assesses whether alternative versions yield comparable scores and interpretations (Hilger & Beauducel, 2017; Lord, 1983). In the present study, we examined whether on-demand, AI-generated parallel items (Form C) for the STARC-5 Emotional Resilience scale achieve psychometric quality comparable to a fixed human-authored parallel form (Form B) and the original form (Form A). Criterion validity was evaluated using the Perceived Stress Questionnaire (PSQ-20), a validated measure of perceived stress in German samples and clinical/healthy populations (Fliege et al., 2001, 2005). We hypothesized that Form C would (H1) show strong parallel-form reliability with A and B (r and ICC agreement >= .70), (H2) demonstrate acceptable internal consistency (alpha and omega >= .70), (H3) show negative criterion correlations with PSQ-20 of at least |.30|, and (H4) yield mean scores equivalent to A and B within plus/minus 0.25 SD.

## Method

### Participants

The analytic sample included 53 respondents. Sex was 67.9% female (n = 36), 30.2% male (n = 16), and 1.9% diverse (n = 1). Education levels (coded 1-7) were distributed as: 1 (n = 1), 2 (n = 1), 3 (n = 19), 4 (n = 6), 5 (n = 6), 6 (n = 16), 7 (n = 4). Employment status (coded 0-3) was 24.5% not employed (n = 13), 13.2% part-time (n = 7), 43.4% full-time (n = 23), and 18.9% self-employed (n = 10). Age was reported in categories ranging from 19 to >67 years; the most frequent categories were 20 (n = 8) and 55 (n = 5). All participants indicated serious participation (ernsthaftigkeit = 1), so no exclusions were applied.

### Measures

STARC-5 Emotional Resilience was administered in three parallel forms with 7 items each (A: original; B: human-generated parallel; C: AI-generated on-demand). Items used a 1-5 response scale; form scores were recomputed as the mean of the 7 items. The PSQ-20 (20 items; 1-4 response scale) served as the criterion measure; items 1, 4, 6, 8, 12, 14, 16, and 19 were reverse-scored, and the PSQ total was recomputed as the mean of the 20 items (Fliege et al., 2001, 2005).

### Procedure

Participants completed all three STARC-5 forms in randomized order (ABC, ACB, BAC, BCA, CAB, CBA) and the PSQ-20 in an online study. Form C items were generated individually for each participant using GPT-5.

### Analytic Strategy

All scale scores were recomputed from item responses, and reverse-coded PSQ-20 items were recalculated. Internal consistency was estimated with Cronbach's alpha and McDonald's omega. Parallel-form reliability was assessed using Pearson correlations among form totals and ICC(2,1) absolute agreement for each form pair. Criterion validity was evaluated with Pearson correlations between each form total and PSQ-20. Mean equivalence was tested using paired TOST on difference scores (A-B, A-C, B-C), with equivalence bounds set to plus/minus 0.25 of the SD of each pairwise difference (AB: ±0.08; AC: ±0.1025; BC: ±0.08). Two one-sided one-sample t-tests were conducted per pair at alpha = .05 with 90% confidence intervals. Missingness was 0% across analyzed variables.

## Results

### Descriptive Statistics

Table 1 presents descriptive statistics for form totals and PSQ-20. Form B had the highest mean, and Form C the lowest mean. Study completion time averaged 321.51 seconds (SD = 247.31).

**Table 1**

*Descriptive Statistics for Primary Study Variables (N = 53)*

| Variable | M | SD | Min | Max |
| --- | --- | --- | --- | --- |
| STARC-5 Form A (recomputed) | 3.49 | 0.69 | 1.00 | 5.00 |
| STARC-5 Form B (recomputed) | 3.57 | 0.61 | 1.57 | 5.00 |
| STARC-5 Form C (recomputed) | 3.34 | 0.67 | 1.00 | 5.00 |
| PSQ-20 (recomputed) | 2.36 | 0.52 | 1.15 | 3.60 |
| Duration (seconds) | 321.51 | 247.31 | 100.67 | 1690.13 |

*Note.* Scores are mean item responses. PSQ-20 reverse items were recomputed.

### Preliminary Analyses

Internal consistency for all three forms exceeded the .70 threshold (Table 2), supporting acceptable reliability for each form.

**Table 2**

*Internal Consistency of STARC-5 Forms (k = 7 Items)*

| Form | Alpha | Omega | M | SD |
| --- | --- | --- | --- | --- |
| A | .81 | .83 | 3.49 | 0.69 |
| B | .79 | .82 | 3.57 | 0.61 |
| C | .85 | .86 | 3.34 | 0.67 |

*Note.* Alpha = Cronbach's alpha; Omega = McDonald's omega.

### Hypothesis Testing

**H1 (parallel-form reliability).** Pearson correlations between forms were strong (r = .82-.89), and ICC(2,1) agreement coefficients were .81-.88 (Table 3), supporting H1.

**Table 3**

*Parallel-Form Reliability Between STARC-5 Forms*

| Pair | Pearson r (95% CI) | ICC(2,1) agreement (95% CI) |
| --- | --- | --- |
| A-B | .89 [.81, .93] | .88 [.79, .93] |
| A-C | .82 [.71, .89] | .81 [.69, .88] |
| B-C | .88 [.80, .93] | .83 [.72, .89] |

*Note.* All correlations p < .001; ICCs are two-way random, absolute agreement, single measures.

**H2 (internal consistency).** All forms met the alpha/omega >= .70 criterion (Table 2), supporting H2.

**H3 (criterion validity).** Each form correlated negatively with PSQ-20 (Table 4). All effects were at least |.30|, supporting H3. Form C showed the strongest negative association with PSQ-20.

**Table 4**

*Criterion Validity: Correlations Between STARC-5 Forms and PSQ-20*

| Form | r with PSQ-20 | 95% CI | p |
| --- | --- | --- | --- |
| A | -.34 | [-.56, -.08] | .012 |
| B | -.39 | [-.60, -.14] | .004 |
| C | -.53 | [-.70, -.30] | < .001 |

**H4 (equivalence of means).** TOST results did not support equivalence for any pair (Table 5). For A-B, the lower-bound test was not significant (p = .435). For A-C and B-C, the upper-bound tests were not significant (p = .818 and p = .999), indicating differences exceeded the equivalence bounds.

**Table 5**

*Equivalence Testing (TOST) for Mean Differences Between Forms*

| Pair | Mean diff | SD diff | Bounds (±0.25 SD) | p (lower) | p (upper) | Equivalent? |
| --- | --- | --- | --- | --- | --- | --- |
| A-B | -0.07 | 0.32 | ±0.08 | .435 | .001 | No |
| A-C | 0.15 | 0.41 | ±0.1025 | < .001 | .818 | No |
| B-C | 0.23 | 0.32 | ±0.08 | < .001 | .999 | No |

*Note.* Lower test: mean > -bound. Upper test: mean < bound. Equivalence requires both p values < .05.

### Exploratory Analyses

No additional exploratory analyses were conducted.

## Discussion

### Summary of Findings

AI-generated Form C demonstrated strong internal consistency and parallel-form reliability with Forms A and B, and meaningful negative correlations with PSQ-20, indicating criterion validity. Thus, H1-H3 were supported. However, mean equivalence across forms was not supported: Form C yielded lower average scores than A and B beyond the predefined equivalence bounds. This suggests that while AI-generated items can preserve rank-order reliability and validity, they may shift mean levels, which is important for score interpretation and longitudinal or comparative uses.

### Limitations

The sample size was modest (N=53), and age was recorded in categorical bins, limiting precision. Equivalence bounds were defined from sample SDs and rounded for testing, which may influence TOST sensitivity. One pairwise difference (B-C) showed non-normality in difference scores, and order effects were not modeled. Finally, we did not evaluate measurement invariance or item-level difficulty calibration across forms.

### Implications

On-demand AI-generated items can yield reliable and valid parallel form scores, supporting their potential use in adaptive or security-focused testing contexts. However, mean shifts suggest that calibration or equating is needed before treating forms as interchangeable for absolute score interpretations.

### Future Directions

Future work should use larger, more diverse samples; incorporate item response theory calibration; test measurement invariance across forms; and examine order and context effects. Investigating AI prompt constraints that reduce mean shifts may improve equivalence.

## Conclusion

AI-generated STARC-5 parallel items produced psychometrically strong scores and meaningful criterion validity, but mean-level equivalence to human-authored forms was not achieved. AI-based form generation appears promising for parallel-form reliability, yet calibration is needed when absolute score comparability is required.

## References

Fliege, H., Rose, M., Arck, P., Levenstein, S., & Klapp, B. (2001). Validierung des "perceived stress questionnaire" (PSQ) an einer deutschen stichprobe. *Diagnostica*, *47*(3), 142-152. https://doi.org/10.1026//0012-1924.47.3.142

Fliege, H., Rose, M., Arck, P., Walter, O., Kocalevent, R., Weber, C., & Klapp, B. (2005). The perceived stress questionnaire (PSQ) reconsidered: validation and reference values from different clinical and healthy adult samples. *Psychosomatic Medicine*, *67*(1), 78-88. https://doi.org/10.1097/01.psy.0000151491.80178.78

Hilger, N., & Beauducel, A. (2017). Parallel-forms reliability. *Encyclopedia of Personality and Individual Differences*, 1-3. https://doi.org/10.1007/978-3-319-28099-8_1337-1

Lai, H., Gierl, M., Touchie, C., Pugh, D., Boulais, A., & Champlain, A. (2016). Using automatic item generation to improve the quality of MCQ distractors. *Teaching and Learning in Medicine*, *28*(2), 166-173. https://doi.org/10.1080/10401334.2016.1146608

Lord, F. (1983). Unbiased estimators of ability parameters, of their variance, and of their parallel-forms reliability. *Psychometrika*, *48*(2), 233-245. https://doi.org/10.1007/bf02294018

Shin, J., & Gierl, M. (2024). Automated short-response scoring for automated item generation in science assessments. *The Routledge International Handbook of Automated Essay Evaluation*, 504-534. https://doi.org/10.4324/9781003397618-30

von Davier, M. (2018). Automated item generation with recurrent neural networks. *Psychometrika*, *83*(4), 847-857. https://doi.org/10.1007/s11336-018-9608-y

---

Created with [NLSS](https://github.com/docmh/nlss-demo).
