---
name: metaskill-report
nlss_version: "1.0.0"
---

# Parallel-Form Quality of AI-Generated Resilience Items

`<user-placeholder>`  
`<affiliation-placeholder>`

`<email-placeholder>`

January 1, 2026

## Abstract

This report evaluates whether AI-generated, participant-specific parallel items (Form C) for the STARC-5 Emotional Resilience scale show psychometric quality comparable to a gold-standard form (A) and a human-written parallel form (B). Using a within-subjects online design (N = 53), we examined internal consistency, parallel-forms reliability, criterion validity against the Perceived Stress Questionnaire (PSQ-20), and practical equivalence of mean scores across forms. Results indicate strong internal consistency for all STARC forms (alpha/omega ≥ .79), high parallel-forms correlations (r = .82–.89) and ICC(2,1) agreement (.83), and moderate-to-strong negative correlations with PSQ-20 (r = -.34 to -.53). Mean differences were small but only A–B met the predefined ±0.25 SD equivalence margin under a descriptive check; A–C and B–C differences exceeded this margin. Overall, AI-generated Form C demonstrates psychometric performance comparable to human-generated forms on reliability and criterion validity, with some mean-level differences warranting caution.

*Keywords:* AI-generated items; parallel forms; reliability; resilience; PSQ-20; psychometrics

# Parallel-Form Quality of AI-Generated Resilience Items

## Introduction

Parallel-form reliability provides evidence that alternate item sets assess the same construct with comparable precision and score properties, and is an important criterion for interchangeable test forms and item bank applications (Hilger & Beauducel, 2017, 2020). Recent work suggests that AI can generate assessment items that approach human-written items in psychometric quality, though human oversight and empirical validation remain critical (Terry et al., 2025). For stress-related measures, the Perceived Stress Questionnaire (PSQ) has an established psychometric basis and is a common criterion for related constructs (Fliege et al., 2005; Montero-Marin et al., 2014). The present study evaluates whether an on-demand, participant-specific AI-generated parallel form (Form C) of the STARC-5 Emotional Resilience scale can meet parallel-form reliability and validity expectations relative to a gold-standard form (A) and a fixed human-written parallel form (B).

## Method

### Participants

The final sample included 53 participants who all indicated serious participation (ernsthaftigkeit = 1). No missing values were detected in the analytic variables.

### Measures

**STARC-5 Emotional Resilience (Forms A, B, C).** Each form contains 7 Likert items (1–5). Form A is the gold-standard instrument; Form B is a fixed human-written parallel form; Form C comprises AI-generated, participant-specific parallel items. Scale scores were computed as mean item ratings for each form.

**Perceived Stress Questionnaire (PSQ-20).** Twenty items rated on a 1–4 scale. Reverse-keyed items (1, 4, 6, 8, 12, 14, 16, 19) were re-coded using 5 - item to create new reverse-keyed variables, then a mean score was computed using the corrected items (Fliege et al., 2005).

### Procedure

Participants completed all three STARC forms in randomized order and the PSQ-20 in an online study. Item order within forms followed the study configuration. Form C items were generated on-demand for each participant.

### Analytic Strategy

Analyses were conducted in NLSS (R 4.5.2). Internal consistency was assessed with Cronbach’s alpha and McDonald’s omega. Parallel-form reliability was assessed via Pearson correlations among form means and a two-way random-effects ICC for absolute agreement (ICC[2,1]). Criterion validity was examined via correlations between each STARC form and PSQ-20. Mean equivalence across forms was evaluated using descriptive mean differences compared against a ±0.25 SD margin of within-subject difference scores (descriptive check; not a formal TOST).

## Results

### Descriptive Statistics

**Table 1**

*Scale Descriptives (Mean Scores)*

| Scale | n | M | SD | Min | Max |
| --- | --- | --- | --- | --- | --- |
| STARC A | 53 | 3.49 | 0.69 | 1.00 | 5.00 |
| STARC B | 53 | 3.57 | 0.61 | 1.57 | 5.00 |
| STARC C | 53 | 3.34 | 0.67 | 1.00 | 5.00 |
| PSQ-20 | 53 | 2.36 | 0.52 | 1.15 | 3.60 |

### Preliminary Analyses

**Table 2**

*Internal Consistency of STARC Forms and PSQ-20*

| Scale | k | alpha | omega | Mean (SD) |
| --- | --- | --- | --- | --- |
| STARC A | 7 | .81 | .83 | 3.49 (0.69) |
| STARC B | 7 | .79 | .82 | 3.57 (0.61) |
| STARC C | 7 | .85 | .86 | 3.34 (0.67) |
| PSQ-20 | 20 | .92 | .92 | 2.36 (0.52) |

All three STARC forms showed acceptable to high internal consistency, meeting the ≥ .70 criterion (H2). PSQ-20 reliability was high, supporting its use as a criterion measure.

### Hypothesis Testing

**H1: Parallel-form reliability.** Form C showed strong correlations with Forms A and B (r = .82–.88, p < .001), and absolute-agreement ICC(2,1) across the three forms was .83 (95% CI [0.76, 0.89]). These results support H1.

**Table 3**

*Parallel-Form Reliability Among STARC Means*

| Comparison | r | 95% CI | p | ICC(2,1) | 95% CI |
| --- | --- | --- | --- | --- | --- |
| A–B | .89 | [.81, .93] | < .001 |  |  |
| A–C | .82 | [.71, .89] | < .001 |  |  |
| B–C | .88 | [.80, .93] | < .001 |  |  |
| Overall (A,B,C) |  |  |  | .83 | [.76, .89] |

**H2: Internal consistency.** Cronbach’s alpha and omega for Form C (α = .85, ω = .86) were comparable to Forms A and B (α = .79–.81; ω = .82–.83), supporting H2.

**H3: Criterion validity.** All three STARC forms correlated negatively with PSQ-20, with Form C showing the strongest association (r = -.53). Each correlation exceeded the |.30| threshold, supporting H3.

**Table 4**

*Criterion Validity (STARC vs. PSQ-20)*

| STARC Form | r with PSQ-20 | 95% CI | p |
| --- | --- | --- | --- |
| A | -.34 | [-.56, -.08] | .012 |
| B | -.39 | [-.60, -.14] | .004 |
| C | -.53 | [-.70, -.30] | < .001 |

**H4: Mean equivalence.** Mean differences were small but only A–B met the ±0.25 SD margin under the within-subject difference SD criterion. A–C and B–C exceeded the equivalence margin, so H4 was partially supported.

**Table 5**

*Mean Differences and Equivalence Margin (±0.25 SD of Difference)*

| Difference | Mean diff | SD diff | Margin (0.25 SD) | Within margin? |
| --- | --- | --- | --- | --- |
| A–B | -0.07 | 0.32 | 0.08 | Yes |
| A–C | 0.15 | 0.41 | 0.10 | No |
| B–C | 0.23 | 0.32 | 0.08 | No |

### Exploratory Analyses

No exploratory analyses were requested.

## Discussion

### Summary of Findings

AI-generated Form C demonstrated strong internal consistency, high parallel-form reliability with Forms A and B, and criterion validity comparable to or stronger than the human-written forms. These results indicate that on-demand AI item generation can produce psychometrically defensible parallel forms for emotional resilience assessment. However, mean-level differences between Form C and the other forms exceeded the prespecified equivalence margin under a descriptive check, suggesting that forms may not be fully interchangeable for mean-score comparisons without further calibration.

### Limitations

The equivalence assessment was descriptive rather than a formal TOST equivalence test. The sample size (N = 53) limits power for detecting smaller differences and evaluating subgroup effects. The literature on the STARC-5 instrument itself is limited in the retrieved sources, so theoretical framing relied on general psychometric and parallel-form guidance.

### Implications

For applications emphasizing rank ordering or correlational inference, AI-generated parallel items appear viable. For contexts where mean comparability is crucial (e.g., longitudinal change or group comparisons across forms), additional calibration or item optimization may be needed.

### Future Directions

Future work should implement formal equivalence testing (TOST), examine measurement invariance across forms, and test AI-generated items across diverse populations and constructs.

## Conclusion

The AI-generated STARC-5 Form C shows strong reliability and criterion validity and meets parallel-form reliability criteria, but mean-level equivalence with human forms remains uncertain. AI-generated parallel forms are promising, with cautious use recommended for mean comparisons.

## References

Fliege, H., Rose, M., Arck, P., Walter, O., Kocalevent, R., Weber, C., & Klapp, B. (2005). The perceived stress questionnaire (psq) reconsidered: validation and reference values from different clinical and healthy adult samples. *Psychosomatic Medicine*, *67*(1), 78-88. https://doi.org/10.1097/01.psy.0000151491.80178.78

Hilger, N., & Beauducel, A. (2017). Parallel-forms reliability. *Encyclopedia of Personality and Individual Differences*, 1-3. https://doi.org/10.1007/978-3-319-28099-8_1337-1

Hilger, N., & Beauducel, A. (2020). Parallel-forms reliability. *Encyclopedia of Personality and Individual Differences*, 3410-3413. https://doi.org/10.1007/978-3-319-24612-3_1337

Montero-Marin, J., Piva Demarzo, M., Pereira, J., Olea, M., & García-Campayo, J. (2014). Reassessment of the psychometric characteristics and factor structure of the ‘perceived stress questionnaire’ (psq): analysis in a sample of dental students. *PLoS ONE*, *9*(1), e87071. https://doi.org/10.1371/journal.pone.0087071

Terry, J., Strait, G., Alsarraf, S., Weinmann, E., & Waychoff, A. (2025). Artificial intelligence in scale development: evaluating ai-generated survey items against gold standard measures. *Current Psychology*, *44*(20), 16339-16350. https://doi.org/10.1007/s12144-025-08240-w

---

Created with [NLSS](https://github.com/docmh/nlss-demo).
